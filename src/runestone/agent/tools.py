"""
Tool definitions for the agent using LangChain's @tool decorator.
"""

import asyncio
import json
import logging
from dataclasses import dataclass
from typing import Literal
from urllib.parse import urlparse

from duckduckgo_search import DDGS
from langchain.tools import ToolRuntime
from langchain_core.tools import tool
from pydantic import BaseModel, Field, field_validator

from runestone.db.models import User
from runestone.services.user_service import UserService
from runestone.services.vocabulary_service import VocabularyService
from runestone.utils.merge import deep_merge

logger = logging.getLogger(__name__)

NewsTimeLimit = Literal["d", "w", "m", "y"]


def _is_swedish_source(url: str) -> bool:
    if not url:
        return False
    try:
        netloc = urlparse(url).netloc.split(":")[0].lower()
    except ValueError:
        return False
    return netloc.endswith(".se")


class WordPrioritisationItem(BaseModel):
    """Input for a single word to prioritize."""

    word_phrase: str = Field(..., description="The Swedish word or phrase")
    translation: str = Field(..., description="Translation of the word_phrase (concise)")
    example_phrase: str = Field(..., description="Example sentence in Swedish")

    @field_validator("word_phrase", "translation", "example_phrase", mode="before")
    @classmethod
    def decode_unicode_escapes(cls, v: str) -> str:
        """Fix double-escaped unicode characters often generated by LLMs."""
        if isinstance(v, str) and "\\u" in v:
            try:
                # v.encode('utf-8') converts 'f\\u00f6...' to bytes b'f\\u00f6...'
                # .decode('unicode_escape') interprets \\u00f6 as รถ
                return v.encode("utf-8").decode("unicode_escape")
            except Exception:
                return v
        return v


class WordPrioritisationInput(BaseModel):
    """Input for prioritizing words for learning."""

    words: list[WordPrioritisationItem] = Field(..., description="List of words to prioritize")


@dataclass
class AgentContext:
    """Context passed to agent tools at runtime."""

    user: User
    # we can't use DI of FastAPI here, so had to put the service to context
    user_service: UserService
    vocabulary_service: VocabularyService
    # Lock to prevent concurrent database access from multiple tool calls
    db_lock: "asyncio.Lock"


@tool
async def read_memory(
    runtime: ToolRuntime[AgentContext],
) -> str:
    """
    Read the entire student memory profile.

    Use this tool when you need context about the student to personalize your
    teaching or when asked about what you know about the student.

    Returns:
    A JSON string containing all memory categories:
        - personal_info: goals, preferences, name, background
        - areas_to_improve: struggling concepts, recurring mistakes
        - knowledge_strengths: mastered topics, successful applications
    """
    user_context = runtime.context.user
    user_service = runtime.context.user_service

    logger.info(f"Reading memory for user {user_context.id}")

    # Fetch fresh user data from DB to ensure we don't use stale data
    # if memory was updated in the same conversation turn.
    user = user_service.get_user_by_id(user_context.id)
    if not user:
        return "Error: User not found."

    memory = {
        "personal_info": json.loads(user.personal_info) if user.personal_info else {},
        "areas_to_improve": json.loads(user.areas_to_improve) if user.areas_to_improve else {},
        "knowledge_strengths": json.loads(user.knowledge_strengths) if user.knowledge_strengths else {},
    }

    return json.dumps(memory, indent=2)


@tool
async def update_memory(
    category: Literal["personal_info", "areas_to_improve", "knowledge_strengths"],
    operation: Literal["merge", "replace"],
    data: dict,
    runtime: ToolRuntime[AgentContext],
) -> str:
    """
    Update the student's memory profile with new information.

    Use this tool to store long-term information about the student:
    - personal_info: goals, preferences, name, background
    - areas_to_improve: struggling concepts, recurring mistakes
    - knowledge_strengths: mastered topics, successful applications

    Args:
        category: Which memory category to update
        operation: 'merge' to add/update keys, 'replace' to overwrite entirely
        data: JSON data to store (use descriptive keys like 'grammar_struggles')
        runtime: Tool runtime context containing user and user_service

    Returns:
        Confirmation message
    """
    user = runtime.context.user
    user_service: UserService = runtime.context.user_service

    logger.info(f"Updating memory for user {user.id}: {category}")

    async with runtime.context.db_lock:
        try:
            final_data = data
            if operation == "merge":
                # Get current data to merge with
                current_json = getattr(user, category)
                current_dict = json.loads(current_json) if current_json else {}
                final_data = deep_merge(current_dict, data)

            # Update memory via service
            user_service.update_user_memory(user, category, final_data)
            return f"Successfully updated {category}."
        except Exception as e:
            logger.error(f"Error updating memory for user {user.id}: {e}")
            # Ensure session is rolled back on error to avoid PendingRollbackError
            try:
                user_service.user_repo.db.rollback()
            except Exception:
                pass
            return f"Error updating memory: {str(e)}"


@tool(args_schema=WordPrioritisationInput)
async def prioritize_words_for_learning(
    words: list[WordPrioritisationItem],
    runtime: ToolRuntime[AgentContext],
) -> str:
    """
    Mark words for priority learning. Use when student uses another language
    to express a word or constantly makes errors writing a word.

    For each word:
    - If deleted: restores it and marks for priority
    - If exists: marks for priority
    - If new: creates it with priority flag

    Args:
        words: List of words to prioritize for learning
        runtime: Tool runtime context

    Returns:
        Confirmation message
    """
    user = runtime.context.user
    vocab_service = runtime.context.vocabulary_service

    processed_count = 0
    errors = []

    async with runtime.context.db_lock:
        for word_item in words:
            try:
                vocab_service.upsert_priority_word(
                    word_phrase=word_item.word_phrase,
                    translation=word_item.translation,
                    example_phrase=word_item.example_phrase,
                    user_id=user.id,
                )
                logger.info(f"Processed priority word: {word_item.word_phrase}")
                processed_count += 1
            except Exception as e:
                logger.error(f"Failed to process {word_item.word_phrase}: {e}")
                errors.append(f"{word_item.word_phrase}: {str(e)}")
                # Rollback to keep session healthy
                try:
                    vocab_service.repo.db.rollback()
                except Exception:
                    pass

    if errors:
        error_msg = "; ".join(errors)
        return f"Processed {processed_count} word(s). Errors: {error_msg}"

    return f"Successfully processed {processed_count} word(s) for priority learning."


MAX_NEWS_TO_FETCH = 10


@tool("search_news_with_dates")
def search_news_with_dates(
    query: str,
    k: int = 5,
    timelimit: NewsTimeLimit = "m",
    region: str = "se-sv",
    swedish_only: bool = False,
) -> str:
    """
    Search Swedish-language news for a topic within a given time window.

    Args:
        query: Search query in Swedish (recommended) or English
        k: Max number of results to return
        timelimit: "d" (day), "w" (week), "m" (month), "y" (year)
        region: DuckDuckGo region code for localization (default: Swedish)
        swedish_only: If True, only return sources with a .se domain

    Returns:
        A formatted list of news results with title, snippet, source URL, and date.
    """
    try:
        k = max(1, min(k, MAX_NEWS_TO_FETCH))
        results = []
        with DDGS(timeout=20) as ddgs:
            ddgs_results = ddgs.news(
                query,
                max_results=k,
                timelimit=timelimit,
                region=region,
            )

            for item in ddgs_results or []:
                title = item.get("title") or "Untitled"
                snippet = item.get("body") or ""
                url = item.get("url") or ""
                date = item.get("date") or "unknown"

                if swedish_only and not _is_swedish_source(url):
                    continue

                results.append(f"{len(results) + 1}. {title}: {snippet} " f"[source: {url}, date: {date}]")

        if not results:
            return "No news results found for that query and time period."

        return "\n\n".join(results)
    except Exception as e:
        logger.exception("News search failed for query='%s'", query)
        return f"Error searching news: {str(e)}"
